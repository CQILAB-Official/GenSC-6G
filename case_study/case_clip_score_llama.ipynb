{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 03:59:40.428994: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 03:59:40.460448: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 03:59:40.460482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 03:59:40.461313: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 03:59:40.466588: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 03:59:41.097617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902864dbddcb4844b9e6d5a4450d6129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "\n",
    "# Initialize LLaMA model and tokenizer for captioning\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"qresearch/llama-3.1-8B-vision-378\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    ").to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qresearch/llama-3.1-8B-vision-378\", use_fast=True)\n",
    "\n",
    "# Initialize CLIPScore metric\n",
    "clip_score_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "\n",
    "    # Generate caption using LLaMA model\n",
    "    generated_text = model.answer_question(\n",
    "         image, \"Caption the image\", tokenizer, max_new_tokens=128, do_sample=True, temperature=0.3\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, generated_text)\n",
    "    return clip_score.detach().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (79 > 77). Running this sequence through the model will result in indexing errors\n",
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Encountered caption longer than max_position_embeddings=77. Will truncate captions to this length.If longer captions are needed, initialize argument `model_name_or_path` with a model that supportslonger sequences\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 27.607014443296375\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-dinov2-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 27.763186086307872\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-dinov2-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 28.064116371039187\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-vit-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 27.55332503680027\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-vit-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 27.700846541289128\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-resnet50-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 27.90542115045316\n"
     ]
    }
   ],
   "source": [
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-resnet50-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
