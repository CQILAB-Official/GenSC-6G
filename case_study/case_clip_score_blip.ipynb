{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 03:09:37.441657: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-28 03:09:37.474051: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-28 03:09:37.474083: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-28 03:09:37.474882: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-28 03:09:37.480412: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-28 03:09:38.247386: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289d735151f74b829dfe5d9cf7c40fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cqilab/anaconda3/envs/sgrs/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Expanding inputs for image tokens in BLIP-2 should be done in processing. Please follow instruction here (https://gist.github.com/zucchini-nlp/e9f20b054fa322f84ac9311d9ab67042) to update your BLIP-2 model. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 29.861747904979822\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import Blip2ForConditionalGeneration, Blip2Processor\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "\n",
    "# Initialize BLIP-2 model and processor for captioning\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "blip2_processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\", revision=\"51572668da0eb669e01a189dc22abe6088589a24\")\n",
    "blip2_model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", revision=\"51572668da0eb669e01a189dc22abe6088589a24\"\n",
    ").to(device)\n",
    "\n",
    "# Initialize CLIPScore metric\n",
    "clip_score_metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch16\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-dinov2-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 30.041159531564425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-dinov2-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 29.974704021396057\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-vit-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 30.09947035962885\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-vit-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 29.99231551921729\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-resnet50-snr10/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CLIP Score: 30.06799986290209\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Directory containing PNG images\n",
    "image_dir = \"logs/upsample-featup/log-resnet50-snr30/50/output\"\n",
    "image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith('.png')]\n",
    "\n",
    "# Function to compute CLIP score for an image\n",
    "def compute_clip_score(image_path):\n",
    "    # Open image and preprocess it\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = np.array(image).transpose(2, 0, 1)  # HWC -> CHW\n",
    "    image_tensor = torch.tensor(image_tensor).unsqueeze(0).float()  # Add batch dimension\n",
    "    image_tensor = image_tensor.to(device)  # Ensure image tensor is on the same device as model\n",
    "    \n",
    "    # Generate caption using BLIP-2 model\n",
    "    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    caption_ids = blip2_model.generate(**inputs)\n",
    "    caption = blip2_processor.decode(caption_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Ensure the caption is a list (since the CLIPScore metric expects a list)\n",
    "    caption = [caption]\n",
    "\n",
    "    # Calculate CLIP score\n",
    "    clip_score = clip_score_metric(image_tensor, caption)\n",
    "    return clip_score.detach().item()\n",
    "\n",
    "# Calculate CLIP scores for all images and compute the average\n",
    "clip_scores = []\n",
    "for image_path in image_paths:\n",
    "    clip_score = compute_clip_score(image_path)\n",
    "    if clip_score is not None:\n",
    "        clip_scores.append(clip_score)\n",
    "\n",
    "if clip_scores:\n",
    "    avg_clip_score = np.mean(clip_scores)\n",
    "    print(f\"Average CLIP Score: {avg_clip_score}\")\n",
    "else:\n",
    "    print(\"No valid clip scores computed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgrs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
